{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import f1_score, classification_report, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Function to calculate Haversine distance\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# Features\n",
    "def preprocess_data(df, is_train=True):\n",
    "\n",
    "    df['trans_date'] = pd.to_datetime(df['trans_date'])\n",
    "    df['dob'] = pd.to_datetime(df['dob'])\n",
    "    df['trans_year'] = df['trans_date'].dt.year\n",
    "    df['trans_month'] = df['trans_date'].dt.month\n",
    "    df['trans_day'] = df['trans_date'].dt.day\n",
    "    df['trans_hour'] = pd.to_datetime(df['trans_time'], format='%H:%M:%S').dt.hour\n",
    "    df['trans_dayofweek'] = df['trans_date'].dt.dayofweek\n",
    "    df['is_weekend'] = df['trans_dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df['age'] = (df['trans_date'] - df['dob']).dt.days // 365\n",
    "    df['amt_log'] = np.log1p(df['amt'])  \n",
    "    df['amt_squared'] = df['amt'] ** 2  \n",
    "    df['hour_amt_interaction'] = df['trans_hour'] * df['amt']\n",
    "    df['distance'] = haversine(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n",
    "\n",
    "    # Drop old columns\n",
    "    drop_cols = ['trans_num', 'trans_date', 'trans_time', 'unix_time', 'first', 'last', 'street',\n",
    "                 'city', 'state', 'zip', 'dob', 'merchant', 'lat', 'long', 'merch_lat', 'merch_long']\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    categorical_cols = ['category', 'gender', 'job']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "    if is_train:\n",
    "        return df.drop(columns=['is_fraud']), df['is_fraud']\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "# Load Data\n",
    "train = pd.read_csv(\"/Users/kenneytran/Downloads/CS506EC/train.csv\")\n",
    "test = pd.read_csv(\"/Users/kenneytran/Downloads/CS506EC/test.csv\")\n",
    "\n",
    "# Preprocess train and test data\n",
    "X, y = preprocess_data(train)\n",
    "X_test = preprocess_data(test, is_train=False)\n",
    "\n",
    "# Feature Selection: Variance Threshold\n",
    "variance_threshold = 0.1\n",
    "selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X = selector.fit_transform(X)\n",
    "X_test = selector.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame after VarianceThreshold\n",
    "selected_features = train.drop(columns=['is_fraud']).columns[selector.get_support()]\n",
    "X = pd.DataFrame(X, columns=selected_features)\n",
    "X_test = pd.DataFrame(X_test, columns=selected_features)\n",
    "\n",
    "# Add Anomaly Scores (IsoForest and LOF)\n",
    "# Apply Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
    "X['IsoForest_Score'] = iso_forest.fit_predict(X)\n",
    "\n",
    "# Apply Local Outlier Factor\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01, novelty=True)\n",
    "lof.fit(X)\n",
    "X['LOF_Score'] = lof.decision_function(X)\n",
    "\n",
    "# Add anomaly features to test data\n",
    "X_test['IsoForest_Score'] = iso_forest.predict(X_test)\n",
    "X_test['LOF_Score'] = lof.decision_function(X_test)\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), X.columns.tolist())  # Scale all features including anomaly scores\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create the full pipeline with SMOTE\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# RandomizedSearchCV for Hyperparameter Tuning\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': [100, 200, 300, 500],\n",
    "    'classifier__max_depth': [3, 5, 7, 9, 12],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    scoring=scorer,\n",
    "    n_iter=100,  \n",
    "    cv=10,  \n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    pre_dispatch='2*n_jobs'\n",
    ")\n",
    "\n",
    "# Perform Randomized Search\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best model and parameters\n",
    "best_pipeline = random_search.best_estimator_\n",
    "print(\"\\nBest Parameters from RandomizedSearchCV:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the model with cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_f1_scores = cross_val_score(best_pipeline, X, y, cv=skf, scoring=scorer)\n",
    "\n",
    "print(\"\\nCross-Validation F1-Scores (Best Model):\", cv_f1_scores)\n",
    "print(f\"Mean F1-Score: {np.mean(cv_f1_scores):.4f}\")\n",
    "print(f\"Standard Deviation of F1-Score: {np.std(cv_f1_scores):.4f}\")\n",
    "\n",
    "# Train the final model on all training data\n",
    "best_pipeline.fit(X, y)\n",
    "\n",
    "# Feature Importance from the XGBoost model\n",
    "feature_importances = best_pipeline.named_steps['classifier'].feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# Save predictions to submission file\n",
    "submission = test[['id']].copy()\n",
    "submission['is_fraud'] = y_test_pred\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nSubmission file saved as 'submission.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
